---
title: "When does passive learning improve the effectiveness of active learning?"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Kyle MacDonald} \\ \texttt{kyle.macdonald@stanford.edu} \\ Department of   Psychology \\ Stanford University
        \And {\large \bf Michael C. Frank} \\ \texttt{mcfrank@stanford.edu} \\ Department of Psychology \\ Stanford University}

abstract: >
    Much of what we learn comes from a mix of information that we select (active) and information that we receive (passive). But which type of training is better for different kinds of learning problems? Here, we explore this question by comparing different sequences of active/passive training in an abstract concept learning task. First, we replicate the active learning advantage from Markant & Gureckis (2014) (Experiments 1a and 1b). Then, we provide a test of whether experiencing active learning first or passive learning first improves the effectiveness of concept learning (Experiment 2). Across both experiments, active training led to better learning of the target concept, but "passive-first" learners were more accurate than "active-first" learners and more efficient than "active-only" learners. These findings broaden our understanding of when different sequences of active/passive learning are more effective, suggesting that for certain problems active exploration can be enhanced with prior passive experience.
    
keywords:
    "active learning, concept learning, replication"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=T, message=F, sanitize = T)

source("../../analysis/helpers/useful.R")
library(compute.es)
library(pwr)
library(langcog)
library(dplyr)
library(magrittr)
library(directlabels)
library(png)
library(grid)
library(ggplot2)
library(xtable)
theme_set(theme_bw())
```

```{r data}
df <- read.csv("../../data/act-learn-all-data-tidy.csv", stringsAsFactors = F)
df <- filter(df, experiment != "prior-manipulation")
```

```{r data_cleaning}
# Rename conditions and reorder levels of the condition factor.
df %<>% mutate(condition_long = condition,
               condition = factor(condition),
               condition_short = factor(condition),
               category_type_string = category_type,
               framing_condition_string = framing_condition,
               category_type = factor(category_type),
               odb_scale = ifelse(odb_scale == "radius_scale", "radius", 
                                  ifelse(odb_scale == "orientation_scale", "orientation",
                                         odb_scale)))

# rename levels of condition variable
levels(df$condition) <- c("AA", "AR", "RA", "RR", "YY")
df$condition <- factor(df$condition, levels = c("AA", "RA", "AR", "RR", "YY"))

# rename and reorder levels of category type variable
levels(df$category_type) <- c("Info-Integration", "Rule-Based")
df$category_type <- factor(df$category_type, levels = c("Rule-Based", "Info-Integration")) 
levels(df$framing_condition) <- c("Info-Integration", "None", "Rule-Based")

# rename levels of condition_short variable
levels(df$condition_short) <- c("A", "AR", "RA", "R", "Y")
df$condition_short <- factor(df$condition_short, levels = c("A", "RA", "AR", "R", "Y"))
```

```{r exp1a-replication-data}
# reb 1a data
df_rep <- df %>% 
    filter(experiment == "replication_1a", 
           condition_short %in% c("A","R"), 
           category_type == "Rule-Based")

# reb 1b data
df_rep1b <- df %>% 
    filter(experiment == "replication_1b", 
           condition_short %in% c("A","R","Y")) 

# sampling data from both replications
df_rep_all <- df %>% filter(condition =="AA", category_type == "Rule-Based")
df_sampling_exp1 <- df %>% filter(condition == "AA", category_type == "Rule-Based", 
                                  trial_type == "training")

# sequence data
df_sequence <- df %>% filter(experiment == "sequence",
                             condition %in% c("RA", "AR"))

# sequence data with rep 1a and 1b data
df_sequence_rep <- df %>% filter(block == 1 | block == 2, 
                                 condition %in% c("RA", "AR", "RR", "AA"))
```

# Introduction

Much of real-world learning occurs in contexts that contain *both* active and passive input. People rarely learn a new concept entirely from information generated by themselves (active learning[^1]) or entirely from information received from the world (passive learning). And yet we do not have a theory about whether different sequences of active/passive input are better for different kinds of learning problems. Consider a teacher introducing a challenging math concept: should she allow students to explore first and then provide instruction, or should she teach first and then let students actively explore?

[^1]: Here we focus on deliberate decisions about what to learn, as opposed to other uses of the term "active" learning (e.g., being engaged with learning materials).

The potential benefits of active learning have been the focus of much research in education [@grabinger1995rich], machine learning [@settles2012active], and cognitive science [@castro2009human]. In a review of this diverse literature, @gureckis2012self suggest that active learning can be superior to passive learning because it allows people to use their prior experience and current hypotheses to select the most helpful examples (e.g., asking a question about something that is particularly confusing). But is active learning always better than passive learning?

@gureckis2012self emphasize that the quality of active exploration is fundamentally linked to the the learner's understanding of the task: if the representation is poor, then self-directed learning will be biased and ineffective. The potential for bias in active learning suggests that receiving passive training first might be especially important for less constrained learning tasks where people are unlikely to generate examples that help them learn the target concept. For example, work by @klahr2004equivalence shows that elementary school-aged children are less effective at discovering the principles of well-controlled experiments from their own self-directed learning. Moreover, @markant2014better showed that active exploration provided no benefit over passive input when there was a mismatch between the target concept and learners' prior hypotheses. In both of these cases, receiving passive training first might have provided learners with a stronger task understanding that enhanced their active exploration. In fact, recent work by @thai2015making has shown that receiving passive examples prior to performing active classifications can enhance perceptual classification learning.

In contrast, experiencing active learning first might be better for tasks with smaller problem spaces where helpful strategies can be extracted via active exploration and generalized to the passive learning context. Education research on the concept of *productive failure* shows that allowing students to struggle with a task (e.g., self-directed problem solving) can lead to better uptake of subsequent instruction [@westermann2012delaying]. And work from cognitive science shows that completing a block of active learning prior to passive learning led to better word learning performance [@kachergis2013actively]. @kachergis2013actively suggest that active-first learners were able to develop attention and memory strategies during active training that generalized to the their passive learning experience.

In the current work, we aim to provide a direct test of how different sequences of active/passive training affect abstract category learning. We use a well-understood perceptual category learning paradigm from @markant2014better since this task produces a reliable active learning advantage. We predicted that passive-first training would be better than active-first training for two reasons. First, we thought that people would generate hypotheses during passive learning, which they could then refine with highly informative examples during active exploration. Second, @markant2014better reported that the quality of active learning was suboptimal early in their experiment, presumably because people were building a better understanding of the learning task. Thus, we predicted that providing passive learning first would allow people to maximize the selection of high quality evidence during active learning and improve their performance. 

```{r stimuli_exp1, fig.env = "figure", fig.pos = "t", fig.width=3.3, fig.height=1.5, fig.align='center', fig.cap = "An example of a training trial from the active learning condition in Experiments 1 and 2. Participants could rotate the antenna or change the antenna's size before seeing which channel the antenna received."}
grid::grid.raster(png::readPNG("figs/stimuli1.png"))
```

# Experiment 1a

Experiment 1a is a direct replication of the active learning advantage for the Rule-Based (RB) category structure found in @markant2014better. In this task, people are asked to learn acategory boundary that is defined by a point on a single continuous dimension (e.g., size). Active learners generate their own examples, whereas passive learners see data generated randomly from the category distributions. We used the same stimuli and followed the exact procedures as the original study with only minor differences, which we describe below. All of the stimuli, the experiments, and analysis code can be viewed and downloaded at the project page for this paper: https://kemacdonald.github.io/Act-Learn/. 

## Methods

### Participants

```{r rep1a-demographics}
demographics_replication <- df_rep %>% 
  distinct(subids) %>% 
  group_by(condition_long, category_type) %>% 
  summarise(count = n())

dnu_exp1a <- df_rep %>% distinct(subids) %>% count(understand) %>% filter(understand == "No")

n_active_rep_1a <- demographics_replication$count[1]
n_passive_rep_1a <- demographics_replication$count[2]
```

We planned a sample of 48 participants, 24 in each condition and posted a set of Human Intelligence Tasks (HITs) to Amazon Mechanical Turk. Only participants with US IP addresses and a task approval rate above 85% were allowed to participate, and each HIT paid one dollar. Data were excluded if participants completed the task more than once or if they reported to not understand the task at the end of the experiment (`r dnu_exp1a$n` HITs). The final sample consisted of `r (n_passive_rep_1a + n_active_rep_1a) - dnu_exp1a$n` participants (Passive: `r n_passive_rep_1a - dnu_exp1a$n`; Active: `r n_active_rep_1a`). 

```{r category_exp1, fig.env = "figure", fig.pos = "t", fig.width=3.3, fig.height=2.5, fig.align='center', fig.cap = "Example distributions of training stimuli shown to participants in the passive learning condition. Each point represents a different antenna constructed with an orientation value (vertical axis) and radius value (horizontal axis). The color of the points show the category membership of each antenna (red: channel 1; blue: channel 2). The solid line in each facet represents the optimal category boundary for both the Rule-Based category and the Information-Integration category structures."}

grid::grid.raster(png::readPNG("figs/category.png"))
```

```{r exp1a_ss_level_summary}
# compute mean accuracy for each participant
ss_rep <- df_rep %>%
    filter(trial_type == "test", understand != "No") %>%
    group_by(condition_short, category_type, subids) %>%
    summarise(ss_accuracy = mean(correct))

# get the cutpoints for +/- 3sd from mean of that group
sd_cut_points <- ss_rep %>% 
    group_by(condition_short, category_type) %>% 
    summarise(minus_3sd = mean(ss_accuracy) - (3 * sd(ss_accuracy)),
              plus_3sd = mean(ss_accuracy) + (3 * sd(ss_accuracy)))

# join cut points with ss level data
ss_rep %<>% left_join(., sd_cut_points, by = c("condition_short", "category_type"))

# get subids removed by filter
ss_remove_exp1a <- ss_rep %>%  
    filter(ss_accuracy <= minus_3sd | ss_accuracy >= plus_3sd) %>% 
    select(subids, ss_accuracy)

# filter data at trial level and ss level, note the use of ! to negate the %in% function
df_rep %<>% filter(!(subids %in% ss_remove_exp1a$subids))
ss_rep %<>% filter(ss_accuracy >= minus_3sd, ss_accuracy <= plus_3sd)
```

```{r exp1a_acc_plot, fig.env = "figure*", fig.pos = "t", fig.width=4.5, fig.height=2.5, fig.align='center', fig.cap = "The left panel shows overall accuracy performance on the classification task for the Active and Passive training conditions from Experiment 1a. The right panel shows accuracy across each of the six blocks in the experiment. The grey curves are generated by a logarithmic smoother and the error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap."}
ms_rep <- ss_rep %>% 
    group_by(condition_short, category_type) %>% 
    summarise(mean_accuracy = mean(ss_accuracy), # get mean accuracy and CIs for group performance
              ci_high = ci.high(ss_accuracy),
              ci_low = ci.low(ss_accuracy))

ms_rep_block <- df_rep %>%
    filter(trial_type == "test", understand != "No") %>%
    group_by(condition_short, category_type, block, subids) %>%
    summarise(ss_accuracy = mean(correct)) %>% 
    group_by(condition_short, category_type, block) %>%
    summarise(mean_accuracy = mean(ss_accuracy),
              ci_high = ci.high(ss_accuracy),
              ci_low = ci.low(ss_accuracy))

levels(ms_rep$condition_short) <- c("Active", "RA", "AR", "Passive", "Y")

acc_plot_1a <- ggplot(aes(x=condition_short, y=mean_accuracy), data=ms_rep) + 
    geom_bar(stat = "identity", aes(fill = condition_short)) + 
    geom_linerange(aes(ymin=mean_accuracy - ci_low, 
                       ymax=mean_accuracy + ci_high), 
                   size=0.6,
                   position=position_dodge(width=0.9)) + 
    geom_hline(yintercept = 0.5, linetype = "dashed") +
    facet_grid(.~category_type) +
    coord_cartesian(ylim=c(0.6,1.0)) +
    scale_fill_solarized() +
    ylab("Mean Accuracy") +
    xlab("Condition") +
    facet_grid(.~category_type) +
    theme(text = element_text(size=16)) +
    guides(fill=F) +
    theme_bw()

levels(ms_rep_block$condition_short) <- c("A", "RA", "AR", "P", "Y")

block_plot_exp1 <- ggplot(aes(x=as.numeric(block), y=mean_accuracy, 
                              group = condition_short), 
                          data=filter(ms_rep_block, category_type == "Rule-Based")) +
    stat_smooth(method = "lm", formula = y ~ log(x), se = F, color = "dark grey") +
    geom_pointrange(aes(ymin=mean_accuracy - ci_low, 
                        ymax=mean_accuracy + ci_high,
                        color = condition_short), 
                    size=0.4,
                    position = position_jitter(width = 0.05)) +
    geom_hline(yintercept = 0.5, linetype = "dashed") +
    facet_grid(.~category_type) +
    scale_y_continuous(limits=c(0.6,1.0)) +
    scale_x_continuous(limits=c(0.5,6.5), breaks = seq(1:6)) +
    scale_color_solarized() +
    ylab("Mean Accuracy") +
    xlab("Block") +
    guides(color = F) +
    theme(text = element_text(size=16)) +
    theme_bw()

block_plot_exp1 <- direct.label(block_plot_exp1, list(last.points, hjust = -0.5))
gridExtra::grid.arrange(acc_plot_1a, block_plot_exp1, ncol = 2, widths = c(2,3))
```

### Stimuli

Visual stimuli were black "antennas" on a white background (Fig 1). Each antenna could vary along two continuous dimensions -- radius size or central angle -- and was assigned a value between 1-600. To make sure that participants could not completely rotate the antenna, we limited the rotation to 150 degrees. The smallest radius and angle parameter values were randomized for each participant, so each participant had a unique category boundary. Finally, we used the Rule-Based category structure where the category boundary is defined by a point on a single dimension: either size or central angle. 

Radius and angle values for the 96 passive training trials were generated from two Gaussian distributions with identical mean and covariance parameters as @markant2014better (Fig 2). For test trials, we created a uniform grid of 192 unique test items that covered the entire parameter space. We randomly sampled 8 items from each quadrant to get 32 test trials for each block and randomized the order of training and test trials within each block and for each participant. 

### Design and procedure

Participants saw a total of 288 trials (96 training and 192 test trials) across 6 blocks\footnote{Six blocks is two blocks shorter than the original experiment. We reduced the length to increase our sample size.}. Each block consisted of 16 training and 32 test trials. Before the task, participants were told that they would see "loop antennas" for televisions and each antenna received one of two channels, and their goal was to learn the difference between the two types of antennas. We told participants that the antennas could sometimes pick up the wrong channel, and that they should learn what channel is most often received by a particular type of antenna.

After the instructions, participants were randomly assigned to one of the two between-subjects conditions (Active vs. Passive). In the Active condition, participants could design their own antennas. They modified the antenna by clicking and dragging the mouse from left to right. To change the size of the antenna, they first pressed the "Z" key. To change the angle, they first pressed the "X" key. When participants were finished with their design, they pressed the space bar to see which channel the antenna received. The channel label appeared in a text box with a green border located above the antenna.

In the Passive condition, participants saw antennas generated randomly using the size and angle values taken from the category distributions. After a two second delay they were told which channel the antenna received. The channel labels were deterministic. To help ensure that participants actually saw the channel, they had to click on the channel label in order to advance the experiment. When they clicked, a green box appeared around the text to indicate that their response was recorded.

After training, participants proceeded to test trials. On each test trial participants saw one antenna and were asked, "Which channel does this antenna receive?" To indicate their response, participants selected one of two buttons (Ch1 or Ch2) located above the antenna. At the end of each block of test trials, participants saw a summary of their accuracy on the preceding block.

## Results and Discussion

### Classification accuracy

```{r exp1a_t-test}
rep1a_t_test <- t.test(ss_accuracy ~ condition_short, var.equal=TRUE, 
                       data=ss_rep)

t.val.km <- rep1a_t_test$statistic
cohens.d.km <- 0.73
```

First we report the planned replication analysis: a t-test to compare overall classification performance between the active and passive conditions. Active learners were more accurate, $t$(`r rep1a_t_test$parameter`) = `r round(rep1a_t_test$statistic, 2)`, $p$ = `r round(rep1a_t_test$p.value, 3)`. Moreover, the effect size of the active learning advantage was large and greater than that of the original study (Original study: $d$ = 0.47; Replication: $d$ = `r cohens.d.km`).

### Classification accuracy over time

```{r mix_model_1a}
mix_model_exp1a <- glmer(correct ~ condition * block + (1|subids), 
                         data=filter(df_rep,trial_type=="test"), 
                         family=binomial,
                         nAGQ = 1) 
```

To quantify performance over time, we used a mixed effect logistic regression including a random effect for participants. We predicted test performance at the trial-level as a function of condition (Active/Passive) and block (1-6) and found a main effect of condition ($\beta$ = `r round(summary(mix_model_exp1a)$coef[2], 2)`, $p$ = `r round(summary(mix_model_exp1a)$coef[2, 4], 3)`) with better performance for active learners overall, and a main effect of block ($\beta$ = `r round(summary(mix_model_exp1a)$coef[3], 2)`, $p$ < .001) such that responses were more accurate later in the experiment. There was also an interaction between block and condition ($\beta$ = `r round(summary(mix_model_exp1a)$coef[4], 2)`, $p$ = `r round(summary(mix_model_exp1a)$coef[4, 4], 2)`) with passive learners showing less of an increase in accuracy across the six test blocks. 

### Relations between quality of active learning and accuracy

```{r avg_samp_dist_trials}
df_sampling_exp1 <- mutate(df_sampling_exp1, 
                           samp_dist_odb = ifelse(odb_scale == "orientation" & category_type == "Rule-Based", 
                                                  abs(as.numeric(odb_param) - orientation_response_param),
                                                  ifelse(odb_scale == "radius" & category_type == "Rule-Based", 
                                                         abs(as.numeric(odb_param) - radius_response_param),
                                                         abs(orientation_response_param - radius_response_param))))
```

```{r avg_samp_dist_subs}
ss_samp_dist_exp1 <- df_sampling_exp1 %>%
    filter(trial_training_block == "active", !(subids %in% ss_remove_exp1a$subids)) %>%
    group_by(subids, condition, block, category_type, experiment) %>%
    summarise(mean_samp_dist = mean(samp_dist_odb))

ss_mean_acc_active_block_exp1 <- df_rep_all %>%
    filter(trial_training_block == "active", trial_type == "test", 
           !(subids %in% ss_remove_exp1a$subids)) %>%
    group_by(subids, condition, block, category_type, experiment) %>%
    summarise(mean_accuracy_active_block = mean(correct))

ss_mean_acc_all_exp1 <- df_rep_all %>%
    filter(trial_type == "test", !(subids %in% ss_remove_exp1a$subids)) %>%
    group_by(subids, condition, category_type, experiment) %>%
    summarise(mean_accuracy = mean(correct))

# join sampling and test acc together
ss_all_exp1 <- left_join(ss_samp_dist_exp1, ss_mean_acc_active_block_exp1, 
                         by=c("subids", "condition", "block", "category_type", "experiment"))

ss_all_exp1 <- left_join(ss_all_exp1, ss_mean_acc_all_exp1, 
                         by=c("subids", "condition", "category_type", "experiment"))
```

```{r exp1a_sampling_model}
lmodel_samp_exp1a <- lm(mean_accuracy ~ mean_samp_dist, 
                        data=filter(ss_all_exp1, experiment == "replication_1a"))
```

Another important result from @markant2014better was the link between the quality of active learning and classification accuracy. Thus, we performed the same analysis on our replication data, quantifying the quality of evidence selection by computing the orthogonal distance between each sample and the true category boundary. Samples closer to the boundary are higher quality. We computed a mean accuracy score and a mean sample distance score for each participant, and fit a linear model using the mean sample distance score to predict accuracy. We found an effect of sample distance ($\beta$ = -.0004, p < .001) with accuracy increasing as the sample distance decreased.

Experiment 1a provides strong evidence for a successful replication of the original results reported in @markant2014better. We found a comparable advantage in accuracy for active learners and we found the same relationship between quality of evidence selection and learning outcomes. Our results differ slightly from the original study in that we found an immediate advantage for active learners in the first block. We next attempt to replicate @markant2014better's findings for the more complex Information-Integration category structure and for the yoked passive learning condition.

# Experiment 1b

The goals of Experiment 1b were: (a) to replicate the lack of an active learning advantage for the more difficult Information-Integration (II) category structure, where the category boundary was defined by a linear combination of an antenna's size and central angle, and (b) to replicate the finding that passive learners did not benefit from being "yoked" to active learners' data.  We also performed an internal replication of the active learning advantage for the RB category that we found in Experiment 1a. We used the same stimuli and followed the exact procedures as the original study. However, we reduced the length of the experiment to two blocks in order to increase our sample size.

```{r rep1b_ss_data}
ss_rep_1b <- df_rep1b %>% 
    filter(trial_type == "test", understand != "No") %>%
    group_by(condition_short, category_type, subids) %>%
    summarise(ss_accuracy = mean(correct))

# get the cutpoints for +/- 3sd from mean of that group
sd_cut_points_1b <- ss_rep_1b %>% 
    group_by(condition_short, category_type) %>% 
    summarise(minus_3sd = mean(ss_accuracy) - (3 * sd(ss_accuracy)),
              plus_3sd = mean(ss_accuracy) + (3 * sd(ss_accuracy)))

# join cut points with ss level data
ss_rep_1b %<>% left_join(., sd_cut_points_1b, by = c("condition_short", "category_type"))

# get subids removed by filter
ss_remove_exp1b <- ss_rep_1b %>%  
    filter(ss_accuracy <= minus_3sd | ss_accuracy >= plus_3sd) %>% 
    select(subids, ss_accuracy)

# filter data at trial level and ss level, note the use of ! to negate the %in% function
df_rep1b %<>% filter(!(subids %in% ss_remove_exp1b$subids))
ss_rep_1b %<>% filter(ss_accuracy >= minus_3sd, ss_accuracy <= plus_3sd)
```
    
```{r rep1b-acc-plot, fig.env = "figure", fig.pos = "t", fig.width=3.3, fig.height=2.5, fig.align='left', fig.cap = "Accuracy across each of the two blocks in Experiment 1b for the Active, Passive, and Yoked training conditions for the Rule-Based (one dimension) and Information-Integration (two dimensions) category structures. Error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap."}
ss_rep_1b_block <- df_rep1b %>% 
    filter(trial_type == "test", understand != "No") %>%
    group_by(condition_short, category_type, subids, block) %>%
    summarise(ss_accuracy = mean(correct))

ms_rep1b_block <- ss_rep_1b_block %>% 
  group_by(condition_short, category_type, block) %>%
  summarise(mean_accuracy = mean(ss_accuracy),
            ci_high = ci.high(ss_accuracy),
            ci_low = ci.low(ss_accuracy))

ggplot(aes(x=as.factor(block), y=mean_accuracy), 
                          data=ms_rep1b_block) +
    geom_pointrange(aes(ymin=mean_accuracy - ci_low, ymax=mean_accuracy + ci_high, color = condition_short), 
                    size=0.4, position = position_jitter(width = 0.11),
                    data = ms_rep1b_block) +
    geom_line(aes(group = condition_short, color = condition_short), size=1, 
              data = ms_rep1b_block) +
     geom_dl(aes(label = condition_short, color = condition_short), 
            data = ms_rep1b_block, 
            method = list(last.bumpup, hjust = -0.5)) +
    facet_grid(.~category_type) +
    scale_y_continuous(limits=c(0.5,0.9)) +
    scale_color_solarized() +
    ylab("Mean Accuracy") +
    xlab("Block") +
    guides(color = F) +
    theme(text = element_text(size=12)) 


ms_rep_1b <- ss_rep_1b %>% 
    group_by(condition_short, category_type) %>% 
    summarise(mean_accuracy = mean(ss_accuracy), # get mean accuracy and CIs for group performance
              ci_high = ci.high(ss_accuracy),
              ci_low = ci.low(ss_accuracy))

levels(ms_rep_1b$category_type) <- c("Rule-Based", "Info-Integration")
levels(ms_rep_1b$condition_short) <- c("A", "RA", "AR", "P", "Y")

a <- ggplot(aes(x=condition_short, y=mean_accuracy), data=ms_rep_1b) + 
    geom_bar(stat = "identity", aes(fill = condition_short)) + 
    geom_linerange(aes(ymin=mean_accuracy - ci_low, 
                       ymax=mean_accuracy + ci_high), 
                   size=0.6,
                   position=position_dodge(width=0.9)) + 
    geom_hline(yintercept = 0.5, linetype = "dashed") +
    facet_grid(.~category_type) +
    coord_cartesian(ylim=c(0.4,1.0)) +
    scale_fill_solarized() +
    ylab("Mean Accuracy") +
    xlab("Condition") +
    facet_grid(.~category_type) +
    theme(text = element_text(size=14)) +
    guides(fill=F) 
```

## Methods

### Stimuli
Visual stimuli were identical to Experiment 1a.

### Participants

```{r rep1b-participants}
demographics_replication_1b <- df_rep1b %>% 
    distinct(subids) %>% 
    xtabs(formula = ~ condition_long + category_type_string) %>% 
    as.data.frame() %>% 
    rename(count = Freq)

dnu_exp1b <- df_rep1b %>% distinct(subids) %>% count(understand) %>% filter(understand == "No")

# get num subs in each condition
nsubs_active_ii_rep_1b <- demographics_replication_1b$count[1]
nsubs_passive_ii_rep_1b <- demographics_replication_1b$count[2]
nsubs_yoked_ii_rep_1b <- demographics_replication_1b$count[3]

nsubs_active_rb_rep_1b <- demographics_replication_1b$count[4]
nsubs_passive_rb_rep_1b <- demographics_replication_1b$count[5]
nsubs_yoked_rb_rep_1b <- demographics_replication_1b$count[6]
```

Participant recruitment and inclusionary/exclusionary criteria were identical to those of Experiment 1a (excluded `r dnu_exp1b$n` HITs). `r sum(demographics_replication_1b$count)` HITs were posted across each of the between-subjects conditions: two category structures and three training conditions (A-RB: `r nsubs_active_rb_rep_1b`, A-II: `r nsubs_active_ii_rep_1b`, P-RB: `r nsubs_passive_rb_rep_1b`, P-II: `r nsubs_passive_ii_rep_1b`, Y-RB: `r nsubs_yoked_rb_rep_1b`, and Y-II: `r nsubs_yoked_ii_rep_1b`).

### Design and procedure

Procedures were identical to those of Experiment 1a except that we included a "yoked" learning condition where we matched each passive learning participant with training data generated by an active learner. Thus, the active and yoked participants saw the exact same data, but active learners were in control of the information flow. We also reduced the length of the experiment to two blocks.

```{r samp_acc_scatter_plot, fig.env = "figure", fig.pos = "t", fig.width=3.3, fig.height=2.5, fig.align='left', fig.cap = "The relations between quality of evidence selection and accuracy on test trials across blocks for participants in Experiments 1a and 1b. Each point is an individual participant. Note that higher mean sample distance means worse evidence selection. The blue lines are linear model fits."}
ss_blocks_1_2 <- filter(ss_all_exp1, block %in% c(1, 2)) %>% 
    mutate(block_string = ifelse(block == 1, "Block 1", "Block 2"))

ggplot(aes(x=mean_samp_dist, y=mean_accuracy_active_block), 
       data = ss_blocks_1_2) +
    geom_point(alpha = 0.4) +
    geom_smooth(method="lm") +
    facet_grid(.~block_string) +
    xlab("Mean Sample Distance") +
    ylab("Mean Accuracy") +
    theme(text = element_text(size=12)) 
```

```{r seq_ss_level_data}
ss_rep_seq <- df_sequence_rep %>% 
    filter(trial_type == "test") %>%
    group_by(condition, category_type, subids) %>%
    summarise(ss_accuracy = mean(correct))

# get the cutpoints for +/- 3sd from mean of that group
sd_cut_points_seq <- ss_rep_seq %>% 
    group_by(condition, category_type) %>% 
    summarise(minus_3sd = mean(ss_accuracy) - (3 * sd(ss_accuracy)),
              plus_3sd = mean(ss_accuracy) + (3 * sd(ss_accuracy)))

# join cut points with ss level data
ss_rep_seq %<>% left_join(., sd_cut_points_seq, by = c("condition", "category_type"))

# get subids removed by filter
ss_remove_seq <- ss_rep_seq %>%  
    filter(ss_accuracy <= minus_3sd | ss_accuracy >= plus_3sd) %>% 
    select(subids, ss_accuracy)

# filter data at trial level and ss level, note the use of ! to negate the %in% function
df_sequence_rep %<>% filter(!(subids %in% ss_remove_seq$subids))
ss_rep_seq %<>% filter(ss_accuracy >= minus_3sd | ss_accuracy <= plus_3sd)
```

```{r exp2_acc_plot, fig.env = "figure", fig.pos = "h", fig.width=3.3, fig.height=2.5, fig.align='left', fig.cap = "Accuracy across each of the two blocks in Experiment 2 for the Passive-Active (PA) and Active-Passive (AP) conditions for the Rule-Based (one dimension) and Information-Integration (two dimensions) category structures. Error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap."}
# acc by block
ss_rep_seq_block <- df_sequence_rep %>% 
    filter(trial_type == "test") %>%
    group_by(condition, category_type, subids, block) %>%
    summarise(ss_accuracy = mean(correct)) 

ms_sequence_block <- ss_rep_seq_block %>% 
    group_by(condition, category_type, block) %>%
    summarise(mean_accuracy = mean(ss_accuracy), # get mean accuracy and CIs for group performance
              ci_high = ci.high(ss_accuracy),
              ci_low = ci.low(ss_accuracy))

levels(ms_sequence_block$condition) <- c("AA", "PA", "AP", "PP", "YY")

ms_sequence_block %<>% 
    mutate(experiment = ifelse(condition %in% c("AA","PP"), 
                               "Experiment 1", "Experiment 2")) %>% 
    filter(condition != "YY") # remove Yoked

b <- ggplot(aes(x=as.factor(block), y=mean_accuracy), 
                          data=ms_sequence_block) +
    geom_pointrange(aes(ymin=mean_accuracy - ci_low, ymax=mean_accuracy + ci_high, color = condition), 
                    size=0.4, position = position_jitter(width = 0.09),
                    data = filter(ms_sequence_block, condition %in% c("AP", "PA"))) +
    geom_line(aes(group = condition, color = condition), size=1, 
              data = filter(ms_sequence_block, condition %in% c("AP", "PA"))) +
     geom_dl(aes(label = condition, color = condition), 
            data = filter(ms_sequence_block, condition %in% c("AP", "PA")), 
            method = list(last.bumpup, hjust = -0.5)) +
    facet_grid(.~category_type) +
    scale_y_continuous(limits=c(0.55,0.9)) +
    scale_color_solarized() +
    ylab("Mean Accuracy") +
    xlab("Block") +
    guides(color = F) +
    theme(text = element_text(size=12)) 

# overall acc
ms_sequence_all <- ss_rep_seq %>% 
    group_by(condition, category_type) %>%
    summarise(mean_accuracy = mean(ss_accuracy), 
              ci_high = ci.high(ss_accuracy),
              ci_low = ci.low(ss_accuracy))

levels(ms_sequence_all$condition) <- c("AA", "PA", "AP", "PP", "YY")
ms_sequence_all %<>% mutate(experiment = ifelse(condition %in% c("AA","PP"), 
                                                "Experiment 1", "Experiment 2"))

a <- ggplot(aes(x=condition, y=mean_accuracy), data=ms_sequence_all) + 
    geom_bar(stat = "identity", aes(fill = condition, alpha = experiment, color = experiment)) + 
    geom_linerange(aes(ymin=mean_accuracy - ci_low, 
                       ymax=mean_accuracy + ci_high), 
                   size=0.6,
                   position=position_dodge(width=0.9)) + 
    geom_hline(yintercept = 0.5, linetype = "dashed") +
    facet_grid(.~category_type) +
    coord_cartesian(ylim=c(0.55,0.9)) +
    scale_fill_solarized() +
    scale_color_manual(values = c("white", "black")) +
    scale_alpha_manual(values=c(0.6,1)) +
    ylab("Mean Accuracy") +
    xlab("Condition") +
    theme(text = element_text(size=16)) +
    guides(fill=F, alpha=F ,color = F) +
    theme_bw()

b

# gridExtra::grid.arrange(b, a, ncol = 2)
```

## Results and Discussion

### Classification accuracy

```{r rep1b_mixed_model}
df_rep1b %<>% mutate(condition = factor(condition)) 


mixed_model_exp1b.3way <- glmer(correct ~ condition * category_type * as.factor(block) + (1|subids), 
                           data=filter(df_rep1b, trial_type=="test"), 
                           nAGQ = 1,
                           control = glmerControl(optimizer = "bobyqa"),
                           family=binomial)
```

We fit the same logistic regression as specified in Experiment 1a and found a main effect of category ($\beta$ = `r round(summary(mixed_model_exp1b.3way)$coef[4], 2)`, p < .001) with better performance in the RB category. We also found a main effect of condition ($\beta$ = `r round(summary(mixed_model_exp1b.3way)$coef[2], 2)`, p < .001) such that participants in the passive and yoked conditions performed worse than participants in the active condition. We also found an interaction between block and the passive learning condition ($\beta$ = `r round(summary(mixed_model_exp1b.3way)$coef[8], 2)`, $p$ = `r round(summary(mixed_model_exp1b.3way)$coef[8,4], 2)`) with passive learners showing more learning over time, and an interaction between category type and block ($\beta$ = `r round(summary(mixed_model_exp1b.3way)$coef[10], 2)`, $p$ = `r round(summary(mixed_model_exp1b.3way)$coef[10,4], 2)`) with less learning in the II category. 

### Relations between evidence selections and accuracy over time

```{r exp1b_sampling_model}
lmodel_sampling_all <- lm(mean_accuracy ~ mean_samp_dist * as.factor(block), 
                          data=filter(ss_all_exp1, block %in% c(1,2)))
```

Since the main goal of the current work was to test the effectiveness of different sequences of active/passive learning, we performed an exploratory analysis of the relations between sampling behavior and test accuracy over time for all active learners in Experiments 1a and 1b. We fit a linear model predicting each participant's mean accuracy based on the quality of their sampling behavior and experiment block. As expected, participants' accuracy improved in the second block ($\beta$ = `r round(summary(lmodel_sampling_all)$coef[3], 2)`, $p$ = `r round(summary(lmodel_sampling_all)$coef[3,4], 2)`). Interestingly, there was a reliable two-way interaction between sample distance and block ($\beta$ = -0.0013, $p$ = `r round(summary(lmodel_sampling_all)$coef[4,4], 3)`) such that the relationship between the quality of evidence selection and accuracy did not emerge until the second block (see Fig 5).

Experiment 1b provides additional evidence for a successful replication of the original results reported in @markant2014better. Importantly, yoked learners performed worse than active learners even though they had seen the same training data, suggesting that the link between people's hypotheses and the input is an important contributor to the active learning advantage. We did find that active learners performed better than both passive and yoked learners in the more complex II category structure. This advantage was not found in the original study, but this finding fits with recent work by @edmunds2015feedback suggesting that RB and II categories are not learned in qualitatively different ways. Finally, we found evidence that the link between the quality of active learning and accuracy emerges as people gain more experience with the task -- an effect that we set out to explore in Experiment 2. 

# Experiment 2

Direct comparisons of active and passive learning are important for understanding when and why we might see advantages for one type of learning over the other. But real-world contexts are not neatly divided into active learning and passive learning. So when are different kinds of learning better for different kinds of learning problems? Experiment 2 provides a direct test of different predictions about the effects of different sequences of active/passive training in an abstract category learning task. Is active-first learning better because it makes people to engage more deeply with the task, enhancing their subsequent passive learning? Or is passive-first training better because it enhances later active learning?

## Methods

### Stimuli

Stimuli were identical to Experiment 1.  

### Participants

```{r participants-seq}
demographics_sequence <- df_sequence %>% 
    distinct(subids) %>% 
    xtabs(formula = ~ condition_long + category_type_string) %>% 
    as.data.frame() %>% 
    rename(count = Freq)

ar.ii <- demographics_sequence[1,3]
ra.ii <- demographics_sequence[2,3]
ar.rb <- demographics_sequence[3,3]
ra.rb <- demographics_sequence[4,3]


dnu_sequence <- df_sequence %>% 
    distinct(subids) %>% 
    count(understand) %>% 
    filter(understand == "No")
```

Participant recruitment and inclusionary/exclusionary criteria were identical to those of Experiment 1 (`r dnu_sequence$n` HITs). Approximately 44 HITs were posted for each training condition and category type for total of `r sum(demographics_sequence$count)` paid HITs (AR-RB: `r ar.rb`, AR-II: `r ar.ii`, RA-RB: `r ra.rb`, RA-II: `r ra.ii`).

### Design and procedure

Procedures were identical to those of Experiment 1. Participants were randomly assigned to one the two between-subjects conditions: Active-Passive (AP) vs. Passive-Active (PA). In the AP condition, participants completed a block of active training and test trials before proceeding to a block of passive learning and test trials. In the PA condition, the order of the training/test blocks was reversed with passive training coming first. 

## Results and Discussion

First, we present analyses of classification accuracy and sampling behavior, focusing on just the AP and PA conditions. The key test of our hypothesis is the interaction between training condition and experiment block, since this tests the effect of training condition on improvement in classification accuracy. We then compare performance in the AP and PA conditions to performance from the Active-Active (AA) and Passive-Passive (PP) conditions from Experiments 1a and 1b, which allows for comparisons to training regimes where learners did not have to switch between types of learning.

### Classification accuracy

```{r mixed-model-seq}
mixed_model_seq.3way <- glmer(correct ~ condition * category_type * as.factor(block) + (1|subids), 
                              data=filter(df_sequence, trial_type=="test"), 
                              nAGQ = 1,
                              control = glmerControl(optimizer = "bobyqa"),
                              family=binomial)
```

Which sequence of active and passive learning was better? We fit a logistic regression predicting test performance based on condition (PA vs. AP), block (1 vs. 2), and category type (RB vs. II). We found a main effect of block ($\beta$ = `r round(summary(mixed_model_seq.3way)$coef[4], 2)`, p < .001), with better performance in the second block across all conditions. For the key test of our hypothesis, we found a reliable two-way interaction between training condition and block ($\beta$ = `r round(summary(mixed_model_seq.3way)$coef[6], 2)`, p < .001), such that participants in the PA condition showed more learning over time compared to the AP condition. We also found a reliable two-way interaction between category type and block ($\beta$ = `r round(summary(mixed_model_seq.3way)$coef[7], 2)`, p < .001) and a marginally significant three-way interaction between condition, category type, and block ($\beta$ = `r round(summary(mixed_model_seq.3way)$coef[8], 2)`, $p$ = `r round(summary(mixed_model_seq.3way)$coef[8,4], 2)`). Participants showed less of an increase in accuracy for the more complex II category, and there was less of an advantage for the PA training condition in the II category.

### Quality of evidence selection

```{r sampling-behavior-seq}
df_sampling_seq <- df_sequence %>% 
    filter(trial_type == "training", condition %in% c("RA", "AR"), understand != "No",
           block == 1 | block == 2) %>% 
    dplyr::select(subids, block, trial_type, condition, 
                  odb_scale, radius_response_param, orientation_response_param,
                  trial_category, order, odb_param, category_type)

df_sampling_seq %<>% mutate(trial_training_block = ifelse(condition == "AR" & block == 1, "Active",
                                                          ifelse(condition == "RA" & block == 2, "Active",
                                                                 ifelse(condition == "AA", "Active",
                                                                        "Receptive"))))

df_sampling_seq %<>% filter(trial_training_block == "Active")

df_sampling_seq <- mutate(df_sampling_seq, 
                          samp_dist_odb = ifelse(odb_scale == "orientation" & category_type == "Rule-Based", 
                                                 abs(as.numeric(odb_param) - orientation_response_param),
                                                 ifelse(odb_scale == "radius" & category_type == "Rule-Based", 
                                                        abs(as.numeric(odb_param) - radius_response_param),
                                                        abs(orientation_response_param - radius_response_param))))

ss_samp_dist_seq <- df_sampling_seq %>%
    filter(trial_training_block == "Active", block == 1 | block == 2) %>%
    group_by(subids, condition, block, category_type) %>%
    summarise(mean_samp_dist = mean(samp_dist_odb))

ss_mean_acc_active_block_seq <- df_sequence %>%
    filter(trial_training_block == "active", trial_type == "test", block == 1 | block == 2) %>%
    group_by(subids, condition, block, category_type) %>%
    summarise(mean_accuracy_active_block = mean(correct))

ss_mean_acc_all_seq <- df_sequence %>%
    filter(trial_type == "test", block == 1 | block == 2) %>%
    group_by(subids, condition, category_type) %>%
    summarise(mean_accuracy = mean(correct))

# join sampling and test acc together
ss_all_seq <- left_join(ss_samp_dist_seq, ss_mean_acc_active_block_seq, 
                        by=c("subids", "condition", "block", "category_type"))

ss_all_seq <- left_join(ss_all_seq, ss_mean_acc_all_seq, 
                        by=c("subids", "condition", "category_type"))
```

```{r sampling-models-seq}
samp_acc_model_seq <- lm(mean_accuracy ~ mean_samp_dist, data=ss_all_seq)

samp_cond_model_seq <- lmer(samp_dist_odb ~ condition * category_type + (1|subids), 
                            data=df_sampling_seq)
```

To test which condition produced higher quality sampling behavior, we fit a linear mixed model predicting sample quality as a function of condition and category type. We found that PA learners generated better samples than AP learners ($\beta$ = `r round(summary(samp_cond_model_seq)$coef[2], 2)`, $t$ = `r round(summary(samp_cond_model_seq)$coef[2,3], 2)`), and we found that sampling quality was lower in the II category ($\beta$ = `r round(summary(samp_cond_model_seq)$coef[3], 2)`, $t$ = `r round(summary(samp_cond_model_seq)$coef[3,3], 2)`). We also we fit a linear model predicting mean classification accuracy based on the mean distance of samples from the target category boundary, and replicate the finding from Experiments 1a and 1b: that higher quality sampling was related to higher accuracy scores ($\beta$ = .00006, $p$ = `r round(summary(samp_acc_model_seq)$coef[2,4], 2)`). 

### Classification accuracy for all sequences

```{r mixed-model-seq-full}
# effect code (choose contrasts based on how you want to interpret model output)
df_sequence_rep %<>% mutate(category_type = factor(category_type),
                            condition = factor(condition),
                            block_factor = factor(block))

contrasts(df_sequence_rep$category_type) <- cbind("base=rb" = c(-1, 1))
contrasts(df_sequence_rep$block_factor) <- cbind("base=block1" = c(-1, 1))
contrasts(df_sequence_rep$condition) <- cbind("active_vs_passive" = c(1, 1, 1, -3), 
                                              "active2_vs_active1" = c(2, -1, -1, -0),
                                              "ra_vs_ar" = c(0, -1, 1, 0))

mixed_model_seq_full <- glmer(correct ~ condition * category_type * block_factor + (1|subids), 
                              data=filter(df_sequence_rep, trial_type=="test"), 
                              nAGQ = 1,
                              control = glmerControl(optimizer = "bobyqa"),
                              family=binomial)
```

How does learning from different sequences of active/passive learning compare to learning from only active or passive data? We fit a logistic regression predicting test performance with the same specifications, but we included data from the active and passive learning conditions in Experiments 1a and 1b and coded user-defined contrasts to test classification accuracy for specific comparisons of interest. We found that receiving any active learning (AA, PA, AP) was better than receiving only passive learning (PP) ($\beta$ = `r round(summary(mixed_model_seq_full)$coef[2], 2)`, $p$ < .01), and that completing two blocks of active learning (AA) was marginally better than completing one block of active learning (PA or AP) ($\beta$ = `r round(summary(mixed_model_seq_full)$coef[3], 2)`, $p$ = `r round(summary(mixed_model_seq_full)$coef[3,4], 2)`). 

### Time costs associated with different learning sequences

```{r training_time_model}
df_train_time <- df %>% 
    filter(trial_type == "training", 
           (block == 1 & condition == "AR") | 
               (block == 2 & condition == "RA") |
               (block %in% c("1", "2") & condition %in% c("AA", "RR"))) %>% 
    mutate(condition = factor(condition),
           rt_sec = rt / 1000)
    
contrasts(df_train_time$condition) <- cbind("active_vs_passive" = c(1, 1, 1, -3), 
                                              "active2_vs_active1" = c(2, -1, -1, -0),
                                              "ra_vs_ar" = c(0, -1, 1, 0))

train_time_lmer <- lmer(rt_sec ~ condition * category_type + (1|subids), data = df_train_time)
```

```{r train_time_plot, eval = F}
ss_train_time <- df %>% 
    filter(trial_type == "training", 
           condition %in% c("AA", "RA", "AR", "RR"), 
           block %in% c(1, 2)) %>% 
    mutate(condition = factor(condition),
           rt_sec = rt / 1000) %>% 
    group_by(condition, block, subids) %>% 
    summarise(ss_rt = mean(rt_sec, na.rm = T))

ms_train_time <- ss_train_time %>% 
    group_by(condition, block) %>% 
    langcog::multi_boot_standard(column = "ss_rt") 

time_plot <- ggplot(data = filter(ms_train_time, 
                                  (block == 1 & condition == "AR") | 
                                      (block == 2 & condition == "RA")) , 
                                  aes(x = condition, y = mean, fill = condition)) +
    geom_bar(stat = "identity") +
    geom_linerange(size = 1, aes(ymin = ci_lower, ymax = ci_upper)) +
    ylab("Mean Training Time (sec)") +
    xlab("Condition") +
    ylim(0,19) +
    scale_fill_solarized() +
    theme(text = element_text(size=20)) +
    guides(fill=F, alpha=F ,color = F) 
```

We were also interested in the time costs associated with different sequences of active and passive learning. We fit a linear mixed effects model with the same specifications but predicting the amount of time spent on each training trial. We found that training took longer in any active learning condition compared to passive learning ($\beta$ = `r round(summary(train_time_lmer)$coef[2], 2)`, $t$ = `r round(summary(train_time_lmer)$coef[2,3], 2)`). We also found that people who completed only active training (AA) took longer than people who only completed one block of active learning (PA, AP) ($\beta$ = `r round(summary(train_time_lmer)$coef[3], 2)`, $t$ = `r round(summary(train_time_lmer)$coef[3,3], 2)`). Surprisingly, we found that people in the active-first condition (AP) took longer than people in the passive-first condition (PA) ($\beta$ = `r round(summary(train_time_lmer)$coef[4], 2)`, $t$ = `r round(summary(train_time_lmer)$coef[4,3], 2)`). We did not find any reliable interactions.  

The main findings from Experiment 2 are that despite experiencing the exact same amount of active and passive data, passive-first learners were more accurate overall, produced higher quality sampling behavior, and spent less time on training. These results suggest that receiving passive training first improved learners' active learning by helping them to explore more efficiently.

# General Discussion

Mixtures of active and passive learning are a fundamental feature of real-world learning contexts. But we do not know when different sequences of active/passive input are better for different kinds of learning problems. In the current work, we take a first step towards answering this important question. We found that that receiving passive learning first improved subsequent active exploration and led to better overall performance in an abstract concept learning task. This finding broadens our understanding of the types of learning problems where passive learning might support effective active learning.

Why was passive-first learning better in this kind of abstract concept learning problem? One possibility is passive training allowed people to generate hypotheses about the correct category boundary, which they could begin testing from the very first trial of active learning. We did find that passive-first learners produced higher quality active learning, suggesting that perhaps active-first learners used their initial active learning less effectively. In contrast, we did not see active learning strategies transfer to passive learning, as had been found previously [@kachergis2013actively]. Perhaps we did not see transfer because this task involved incremental changes to hypotheses, which depend on generating the right example to falsify the learner's current hypothesis on a trial by trial basis. In contrast, if active learning helps learners develop better attentional/memory strategies, these strategies can often be transferred to passive contexts.

It is interesting that two blocks of active learning was slightly better than the passive-first learning. But, passive-first learning provides an additional benefit: it reduces learning costs (i.e., time and mental effort). We found that active learners took longer to complete the task and spent more time on the training blocks. Thus, even if passive-first learning does not achieve the same level of performance as active learning, it still might be preferable. 

There are several limitations to our experiment. First, while the learning task we used is well-understood, like other tasks of this type it dramatically simplifies real-world concept learning. So we do not know how our sequencing finding would scale to learning more complex, higher dimensional concepts. It could be that passive learning becomes even more important as the concept becomes more complex. Second, we used a coarse sequence manipulation, at the level of training/test blocks. And sequences of active/passive learning might have differential effects on learning at finer levels of granularity (e.g., trial level).

We need a theory of what kinds of learning work best for the diverse set of problems that learners must solve. Our work here provides an important first step towards understanding when passive learning experiences could be used to support better active exploration. Overall, these results show that, for some tasks, passive learning can equip people with a better task representation, making them more effective active learners. 

# Acknowledgements

We are grateful to Doug Markant and Todd Gureckis for sharing the details and code from the original experiment. We thank the members of the Language and Cognition Lab for their helpful feedback on this project. This work was supported by a National Science Foundation Graduate Research Fellowship to KM.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
