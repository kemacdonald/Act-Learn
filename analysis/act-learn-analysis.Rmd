---
title: "Act-Learn Analysis"
author: "Kyle MacDonald"
date: "September 30, 2015"
output: html_document
---

```{r, echo = F}
rm(list=ls()) # clear workspace
knitr::opts_chunk$set(warning=FALSE, message=FALSE, sanitize = T, 
                      fig.height=6, fig.width=9, echo=F, cache = T)
```


This script analyzes data from two active/passive learning experiments.

* Exact replication of Markant and Gureckis, 2014 JEP 
* An extension testing the effectiveness of different sequences of active vs. passive training 

```{r libraries, warning=F, message=F, comment=F}
source("helpers/useful.R")
library(langcog)
library(dplyr)
library(magrittr)
library(directlabels)
```

```{r}
df <- read.csv("../data/act-learn-all-data-tidy.csv", stringsAsFactors = F)
```

```{r, echo = F, eval = F}
df_comments <- df %>% 
    select(subids, condition, category_type, order, understand, comments) %>% 
    distinct() %>% 
    arrange(condition, category_type, order, understand)
```

```{r}
# Rename conditions and reorder levels of the condition factor.
df %<>% mutate(condition = factor(condition),
               odb_scale = ifelse(odb_scale == "radius_scale", "radius", 
                                  ifelse(odb_scale == "orientation_scale", "orientation",
                                         odb_scale)))

levels(df$condition) <- c("AA", "AR", "RA", "RR", "YY")
df$condition <- factor(df$condition, levels = c("AA", "RR", "RA", "AR", "YY"))
```

```{r}
# Remove participants who reported misunderstanding the task
df %<>% filter(block == 1 | block == 2, condition != "YY")
```

## Descriptives

```{r demographics, echo=FALSE}
demo_df <- df %>% 
    distinct(subids) %>% 
    xtabs(formula = ~ condition + category_type + order) %>% 
    as.data.frame() %>% 
    rename(count = Freq)

demo_df <- df %>% 
    group_by(subids, condition, order, category_type) %>%
    summarise(exp_length_minutes = sum(rt) / 60000) %>% 
    group_by(condition, order, category_type) %>% 
    summarise(mean_exp_length = mean(exp_length_minutes),
              sd_exp_length = sd(exp_length_minutes)) %>% 
    left_join(demo_df, by = c("condition", "order", "category_type")) %>% 
    mutate(participants_needed = 20 - count)
    
knitr::kable(demo_df)
```

Histogram of length of experiment split by condition 

```{r, echo=F}
ss_rt <- df %>%
    group_by(subids, condition) %>%
    summarise(exp_length_min = sum(rt) / 60000)

qplot(exp_length_min, data=ss_rt, facets=.~condition)
```

## Overall accuracy analysis 

Get mean accuracy for each condition and category type

```{r, echo=F}
ms <- df %>%
    filter(trial_type == "test", block == 1 | block == 2, condition != "YY") %>%
    group_by(condition, category_type) %>%
    summarise(mean_accuracy = mean(correct),
              ci_high = ci.high(correct),
              ci_low = ci.low(correct))
```

Plot.

```{r mean acc plot condition/order, echo=F, fig.width=14}
qplot(x=condition, y=mean_accuracy, data=ms,
      geom="bar", stat="identity", fill = condition, position = "dodge") + 
    geom_linerange(aes(ymin=mean_accuracy - ci_low, 
                       ymax=mean_accuracy + ci_high), 
                   width = .05, size=0.6, position=position_dodge(width=0.9)) + 
    coord_cartesian(ylim=c(0.55,0.95)) +
    scale_fill_solarized() +
    ylab("Mean Accuracy") +
    xlab("Condition") +
    facet_grid(.~category_type) +
    theme(text = element_text(size=16)) +
    guides(fill=F) +
    theme_bw()
```

We see the overall advantage for active learning over passive learning across both category types. 

## Accuracy by block analysis

Next, we analyze accuracy across the two blocks.

```{r acc by block, echo=F}
ms_block <- df %>%
    filter(trial_type == "test", block == 1 | block == 2, condition != "YY") %>%
    group_by(condition, block, category_type) %>%
    summarise(mean_accuracy = mean(correct),
              ci_high = ci.high(correct),
              ci_low = ci.low(correct))
```

```{r acc by block plot, echo=F}
block_plot <- qplot(x=block, y=mean_accuracy, data=ms_block, color = condition,
      geom=c("blank"), stat="identity") +
    geom_smooth(method = "lm", se=F) +
    geom_pointrange(aes(ymin=mean_accuracy - ci_low, 
                       ymax=mean_accuracy + ci_high), 
                   width = .05, size=0.6) + 
    scale_x_discrete() +
    scale_y_continuous(limits=c(0.55,0.9)) +
    scale_color_solarized() +
    facet_grid(.~category_type) + 
    ylab("Mean Accuracy") +
    xlab("Block") +
    guides(color = F) +
    theme(text = element_text(size=16)) +
    theme_bw()

direct.label(block_plot, list(last.bumpup, hjust = -0.5))
```

The block analysis shows an effect of order on active learning. 

Receptive-Active learners are more accurate after their block of active learning (block 2) compared to Active-Receptive learners (block 1). 

## Accuracy by block and order 

Order here refers to whether size or angle was the category dimension.

* Order 1, Rule-based is Size
* Order 2, Rule-based is Angle

* Order 1, II is y = x 
* Order 2, II is y = -x


Rename order labels, so they make sense

```{r}
df %<>% mutate(order_rule_based = ifelse(order == "order1" & category_type == "rule-based", "size",
                                         ifelse(order == "order2" & category_type == "rule-based", "angle",
                                                NA)))
```


```{r acc by block and dimension, echo=F}
ms_block_order <- df %>%
    filter(trial_type == "test", block == 1 | block == 2, condition != "YY") %>%
    group_by(condition, block_factor, order, order_rule_based, category_type) %>%
    summarise(mean_accuracy = mean(correct),
              ci_high = ci.high(correct),
              ci_low = ci.low(correct))
```

## Plot accuracy over blocks

### Rule-Based category structure

```{r acc by block and dim plot, echo=F}
block_plot2 <- qplot(x=block_factor, y=mean_accuracy, 
                     data = filter(ms_block_order, category_type == "rule-based"), 
                     color = condition, group = condition,
      geom=c("point", "line"), stat="identity", facets=.~order_rule_based) + 
    geom_linerange(aes(ymin=mean_accuracy - ci_low, 
                       ymax=mean_accuracy + ci_high), 
                   width = .05, size=0.6) + 
    scale_color_solarized() +
    scale_y_continuous(limits=c(0.45,1)) +
    scale_x_discrete() +
    ylab("Mean Accuracy") +
    xlab("Block") +
    theme_bw() +
    theme(text = element_text(size=18)) 
    

direct.label(block_plot2, list(last.bumpup, hjust = -0.5))
```

For the category that depends on size, AA and RA end up on top of each other, whereas AR and RR do not. I'm not sure what's going on with the "angle" category -- Perhaps this is just easier to learn overall and so we are not seeing any condtion differences? 

Also, there seems to be some between subjects variation here -- could this explain why the RR learners are the best in the angle category? Should we try to replicate this order difference?

### Information Integration category structure 

```{r acc by block and dim plot II, echo=F}
block_plot3 <- qplot(x=block_factor, y=mean_accuracy, 
                     data = filter(ms_block_order, category_type == "information-integration"), 
                     color = condition, group = condition,
      geom=c("point", "line"), stat="identity", facets=.~order) + 
    geom_linerange(aes(ymin=mean_accuracy - ci_low, 
                       ymax=mean_accuracy + ci_high), 
                   width = .05, size=0.6) + 
    scale_color_solarized() +
    scale_y_continuous(limits=c(0.45,1)) +
    scale_x_discrete() +
    ylab("Mean Accuracy") +
    xlab("Block") +
    theme_bw() +
    theme(text = element_text(size=18)) 
    
direct.label(block_plot3, list(last.bumpup, hjust = -0.5))
```

## Evidence selection analysis (active learning)

Analyze the average distance of participants' samples from the optimal decision
boundary.

```{r sampling data, echo=F}
df_sampling <- df %>% 
    filter(trial_type == "training", condition %in% c("AA", "RA", "AR"),
           block == 1 | block == 2) %>% 
    dplyr::select(subids, block, trial_type, condition, odb_scale, radius_response_param, orientation_response_param,
           trial_category, order, odb_param, category_type)

df_sampling %<>% mutate(trial_training_block = ifelse(condition == "AR" & block == 1, "active",
                                                      ifelse(condition == "RA" & block == 2, "active",
                                                             ifelse(condition == "AA", "active",
                                                                    "receptive"))))

df_sampling %<>% filter(trial_training_block == "active")
```

Rotate, so orientation and radius are on the same dimension.

* dim of interest = dimension with category boundary
* other dim = dimension that does not contain the category boundary 

```{r rotate samples, echo=F}
df_sampling <- df_sampling %>%
    mutate(dim_of_interest = ifelse(category_type == "rule-based" & odb_scale == "orientation", orientation_response_param,
                                    ifelse(category_type == "rule-based" & odb_scale == "radius", radius_response_param,
                                           radius_response_param)),
           other_dim = ifelse(category_type == "rule-based" & odb_scale == "orientation", radius_response_param,
                              ifelse(category_type == "rule-based" & odb_scale == "radius", orientation_response_param,
                              orientation_response_param)))
```

Plot group level sampling behavior.

```{r sampling plot, fig.width=8}
qplot(x=dim_of_interest, y=other_dim, data = df_sampling,
      facets=category_type~condition)
```

```{r}
df_sampling$dim_ref <- ifelse(df_sampling$dim_of_interest <= 300, 
                              df_sampling$dim_of_interest, 
                              600 - df_sampling$dim_of_interest)
ggplot(data = df_sampling,
       aes(x = dim_ref, fill=condition)) + 
    geom_histogram(aes(y=..density..),
                   position = "dodge",
                   binwidth = 50) +
    xlim(0,300) +
    facet_grid(category_type~block) +
    scale_fill_solarized()
```

Plot individual participant sampling behavior

```{r sampling subs, echo=F, fig.width=12, fig.height=8, eval = F}
qplot(x=dim_of_interest, y=other_dim, data=df_sampling,
      color = condition) +
    facet_wrap(condition~subids) +
    geom_vline(aes(xintercept=300)) 
```

Get distance from optimal decision boundary for each sample.

```{r}
df_sampling <- mutate(df_sampling, 
                      samp_dist_odb = ifelse(odb_scale == "orientation" & category_type == "rule-based", abs(as.numeric(odb_param) - orientation_response_param),
                                             ifelse(odb_scale == "radius" & category_type == "rule-based", abs(as.numeric(odb_param) - radius_response_param),
                                                    abs(orientation_response_param - radius_response_param))))
```

Now get the average distance across subjects

```{r}
ms_sampling <- df_sampling %>%
    group_by(condition, block, category_type) %>%
    summarize(mean_samp_dist = mean(samp_dist_odb, na.rm = T),
              ci_high = ci.high(samp_dist_odb),
              ci_low = ci.low(samp_dist_odb))
```

Plot.

```{r}
qplot(x=condition, y=mean_samp_dist, data=ms_sampling,
      geom="bar", stat="identity", fill = condition, position = "dodge") + 
    geom_linerange(aes(ymin=mean_samp_dist - ci_low, 
                       ymax=mean_samp_dist + ci_high), 
                   width = .05, size=0.6, position=position_dodge(width=0.9)) + 
    scale_fill_solarized() +
    coord_cartesian(ylim=c(100, 275)) +
    ylab("Mean Sample Distance") +
    xlab("Condition") +
    facet_grid(category_type~block) +
    theme(text = element_text(size=16)) +
    theme_bw()
```

Active learning is better after getting a block of receptive learning trials. But not better than getting two blocks of Active learning trials.

## Relationship between sampling and test

Get the mean sample distance and accuracy for each participant.

```{r}
ss_samp_dist <- df_sampling %>%
    filter(trial_training_block == "active", block == 1 | block == 2) %>%
    group_by(subids, condition, block, category_type) %>%
    summarise(mean_samp_dist = mean(samp_dist_odb),
              ci_high_msd = ci.high(samp_dist_odb),
              ci_low_msd = ci.low(samp_dist_odb))

ss_mean_acc_active_block <- df %>%
    filter(trial_training_block == "active", trial_type == "test", block == 1 | block == 2) %>%
    group_by(subids, condition, block, category_type) %>%
    summarise(mean_accuracy_active_block = mean(correct),
              ci_high_acc_active_block = ci.high(correct),
              ci_low_acc_active_block = ci.low(correct))

ss_mean_acc_all <- df %>%
    filter(trial_type == "test", block == 1 | block == 2) %>%
    group_by(subids, condition, category_type) %>%
    summarise(mean_accuracy = mean(correct),
              ci_high_acc = ci.high(correct),
              ci_low_acc = ci.low(correct))

# join sampling and test acc together
ss_all <- left_join(ss_samp_dist, ss_mean_acc_active_block, by=c("subids", "condition", "block", "category_type"))
ss_all <- left_join(ss_all, ss_mean_acc_all, by=c("subids", "condition", "category_type"))
```

Plot

```{r}
a <- qplot(x=mean_samp_dist, y=mean_accuracy_active_block, data = filter(ss_all, block == 1)) +
    geom_smooth(method="lm") +
    facet_grid(condition~category_type) +
    xlab("Mean Sample Distance") +
    ylab("Mean Accuracy") +
    theme(text = element_text(size=16))

b <- qplot(x=mean_samp_dist, y=mean_accuracy_active_block, data = filter(ss_all, block == 2)) +
    geom_smooth(method="lm") +
    facet_grid(condition~category_type) +
    xlab("Mean Sample Distance") +
    ylab("Mean Accuracy") +
    theme(text = element_text(size=16))

gridExtra::grid.arrange(a, b, ncol = 2)
```


## Individual accuracy across blocks: consistency analysis

```{r, fig.height=9, fig.width=14}
ss_mean_acc_explore <- df %>%
    filter(trial_type == "test", block == 1 | block == 2) %>%
    group_by(subids, condition, block, order, category_type) %>%
    summarise(mean_accuracy = mean(correct),
              ci_high_acc = ci.high(correct),
              ci_low_acc = ci.low(correct))
```

Plot.

```{r}
qplot(x=as.factor(block), y=mean_accuracy, data=ss_mean_acc_explore, 
      color = order, group = as.factor(subids)) +
    geom_line() +
    geom_smooth(aes(group=1), method = "lm", se = F, color = "red", size = 2) +
    facet_grid(category_type~condition) +
    xlab("Block") +
    ylab("Mean Accuracy") +
    theme(text = element_text(size=16)) +
    scale_color_solarized()
```

There is a different overall pattern of accuracy performance across blocks by condition. Receptive-first learners show larger growth compared to Active-first learners. 

## Models

### Accuracy on the trial-level based on condition and block

Does condition and block predict accuracy on test trials?

```{r, echo = F, eval = F}
df %<>% mutate(block_factor = factor(block))

m1 <- glmer(correct ~ condition * block_factor * category_type + (1|subids), 
            data=filter(df, trial_type=="test", condition != "YY"), 
            family=binomial, 
            nAGQ = 0,
            control = glmerControl(optimizer = "bobyqa"))

summary(m1)
```

Reliable interaction between condition and block. Receptive-first learners perform better on the second block of test trials than Active-first learners. 

But overall, the two groups are not different from one another. How to interpret? 

### Accuracy based on sampling behavior and condition

Does mean accuracy depend on sampling behavior and condition?

```{r, eval = F}
m2 <- lm(mean_accuracy ~ mean_samp_dist * condition, data=ss_all)
summary(m2)
```

Reliable interaction between mean sample distance and condition. If you get Receptive-first, then better sampling predicts better test, but not if you get Active-first.

### Sampling behavior based on condition

Which condition is "better" at sampling?

```{r, eval = F}
m3_samp <- lmer(samp_dist_odb ~ condition + (1|subids), data=df_sampling)
summary(m3_samp)
```

Receptive-first participants are better at sampling than active first participants.

### Effect coding (condition vs. category type) to test main effects.

effect code (choose contrasts based on how you want to interpret model output)

```{r, eval=T, echo = T}
df %<>% mutate(category_type = factor(category_type),
               condition = factor(condition),
               block_factor = factor(block))

contrasts(df$category_type) <- cbind("base=rb" = c(1, -1))

contrasts(df$condition) <- cbind("active_vs_passive" = c(1, -3, 1, 1), "active2_vs_active1" = c(2, 0, -1, -1),
                                        "ra_vs_ar" = c(0, 0, 1, -1))

contrasts(df$block_factor) <- cbind("base=block1" = c(-1, 1))
```

Model with effect coding. 

```{r, eval = T, echo = T}
m3 <- glmer(correct ~ condition * category_type * block_factor + (1|subids), 
            data=filter(df, trial_type=="test"), 
            nAGQ = 0,
            control = glmerControl(optimizer = "bobyqa"),
            family=binomial)

knitr::kable(summary(m3)$coefficients)
```

Intercept is the mean of the means (or the grand mean) of all the groups. These data are unbalanced.
Active better than passive. Information integration worse than rule-based.